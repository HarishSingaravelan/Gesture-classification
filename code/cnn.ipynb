{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'Device ID', 'Warm?', 'Sync', 'Arm', 'Timestamp',\n",
      "       'Orientation_W', 'Orientation_X', 'Orientation_Y', 'Orientation_Z',\n",
      "       'Acc_X', 'Acc_Y', 'Acc_Z', 'Gyro_X', 'Gyro_Y', 'Gyro_Z', 'Pose',\n",
      "       'EMG_1', 'EMG_2', 'EMG_3', 'EMG_4', 'EMG_5', 'EMG_6', 'EMG_7', 'EMG_8',\n",
      "       'Locked', 'RSSI', 'Roll', 'Pitch', 'Yaw', 'newTimeStamp',\n",
      "       'elapsed_seconds', 'label'],\n",
      "      dtype='object')\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\haris\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.5808 - loss: 0.7272 - val_accuracy: 0.6152 - val_loss: 0.6585\n",
      "Epoch 2/20\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6800 - loss: 0.5841 - val_accuracy: 0.7240 - val_loss: 0.5503\n",
      "Epoch 3/20\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7153 - loss: 0.5485 - val_accuracy: 0.7350 - val_loss: 0.5152\n",
      "Epoch 4/20\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7477 - loss: 0.5062 - val_accuracy: 0.7618 - val_loss: 0.4919\n",
      "Epoch 5/20\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7675 - loss: 0.4859 - val_accuracy: 0.7702 - val_loss: 0.4784\n",
      "Epoch 6/20\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7796 - loss: 0.4555 - val_accuracy: 0.7859 - val_loss: 0.4501\n",
      "Epoch 7/20\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7962 - loss: 0.4289 - val_accuracy: 0.7644 - val_loss: 0.4836\n",
      "Epoch 8/20\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8072 - loss: 0.4226 - val_accuracy: 0.7932 - val_loss: 0.4490\n",
      "Epoch 9/20\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8188 - loss: 0.3923 - val_accuracy: 0.8060 - val_loss: 0.4221\n",
      "Epoch 10/20\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8200 - loss: 0.3823 - val_accuracy: 0.8074 - val_loss: 0.4076\n",
      "Epoch 11/20\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8200 - loss: 0.3790 - val_accuracy: 0.8217 - val_loss: 0.3884\n",
      "Epoch 12/20\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8317 - loss: 0.3630 - val_accuracy: 0.8141 - val_loss: 0.3870\n",
      "Epoch 13/20\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8417 - loss: 0.3552 - val_accuracy: 0.8048 - val_loss: 0.4247\n",
      "Epoch 14/20\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8480 - loss: 0.3401 - val_accuracy: 0.8298 - val_loss: 0.3713\n",
      "Epoch 15/20\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.8478 - loss: 0.3348 - val_accuracy: 0.8421 - val_loss: 0.3503\n",
      "Epoch 16/20\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8511 - loss: 0.3235 - val_accuracy: 0.8330 - val_loss: 0.3903\n",
      "Epoch 17/20\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8512 - loss: 0.3281 - val_accuracy: 0.8424 - val_loss: 0.3653\n",
      "Epoch 18/20\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8610 - loss: 0.3109 - val_accuracy: 0.8546 - val_loss: 0.3364\n",
      "Epoch 19/20\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8647 - loss: 0.3068 - val_accuracy: 0.8519 - val_loss: 0.3340\n",
      "Epoch 20/20\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8645 - loss: 0.3045 - val_accuracy: 0.8519 - val_loss: 0.3397\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8575 - loss: 0.3371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8519\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization, GlobalAveragePooling1D\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"output.csv\"  # Update this with your actual file path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df.columns = df.columns.str.strip()\n",
    "print(df.columns)\n",
    "\n",
    "# Drop non-numeric columns\n",
    "non_numeric_cols = ['Device ID', 'Warm?', 'Sync', 'Arm', 'Timestamp', 'Pose', 'Locked', 'RSSI', 'newTimeStamp']\n",
    "df = df.drop(columns=non_numeric_cols, errors='ignore')\n",
    "\n",
    "# Encode the target labels\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['label'])  # Convert 'release' and 'fisk' to numbers\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(df.drop(columns=['label']))\n",
    "y = tf.keras.utils.to_categorical(df['label'])  # One-hot encode labels\n",
    "\n",
    "# Reshape for CNN (samples, time steps, features)\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build CNN Model\n",
    "model = Sequential([\n",
    "    Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X.shape[1], 1)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    \n",
    "    Conv1D(filters=128, kernel_size=3, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    \n",
    "    # Global Average Pooling instead of Flatten to avoid shape mismatches\n",
    "    GlobalAveragePooling1D(),\n",
    "    \n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(y.shape[1], activation='softmax')  # Output layer for classification\n",
    "])\n",
    "\n",
    "# Compile Model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train Model\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate Model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Save Model\n",
    "model.save(\"gesture_cnn_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\haris\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.6224 - loss: 0.8004 - val_accuracy: 0.7103 - val_loss: 0.5438\n",
      "Epoch 2/20\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7347 - loss: 0.5199 - val_accuracy: 0.7903 - val_loss: 0.4596\n",
      "Epoch 3/20\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7703 - loss: 0.4604 - val_accuracy: 0.8159 - val_loss: 0.4023\n",
      "Epoch 4/20\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8057 - loss: 0.4159 - val_accuracy: 0.8144 - val_loss: 0.4019\n",
      "Epoch 5/20\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8307 - loss: 0.3689 - val_accuracy: 0.8421 - val_loss: 0.3487\n",
      "Epoch 6/20\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8425 - loss: 0.3479 - val_accuracy: 0.8371 - val_loss: 0.3534\n",
      "Epoch 7/20\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8529 - loss: 0.3295 - val_accuracy: 0.8519 - val_loss: 0.3377\n",
      "Epoch 8/20\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8597 - loss: 0.3078 - val_accuracy: 0.8551 - val_loss: 0.3028\n",
      "Epoch 9/20\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8765 - loss: 0.2798 - val_accuracy: 0.8656 - val_loss: 0.2952\n",
      "Epoch 10/20\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8871 - loss: 0.2627 - val_accuracy: 0.8787 - val_loss: 0.2715\n",
      "Epoch 11/20\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8843 - loss: 0.2584 - val_accuracy: 0.8767 - val_loss: 0.2840\n",
      "Epoch 12/20\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8885 - loss: 0.2530 - val_accuracy: 0.8706 - val_loss: 0.3192\n",
      "Epoch 13/20\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8957 - loss: 0.2437 - val_accuracy: 0.8918 - val_loss: 0.2675\n",
      "Epoch 14/20\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8949 - loss: 0.2300 - val_accuracy: 0.8714 - val_loss: 0.3097\n",
      "Epoch 15/20\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9002 - loss: 0.2278 - val_accuracy: 0.8805 - val_loss: 0.2896\n",
      "Epoch 16/20\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9077 - loss: 0.2130 - val_accuracy: 0.8930 - val_loss: 0.2693\n",
      "Epoch 17/20\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9095 - loss: 0.2153 - val_accuracy: 0.8956 - val_loss: 0.2603\n",
      "Epoch 18/20\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9155 - loss: 0.1926 - val_accuracy: 0.8915 - val_loss: 0.2587\n",
      "Epoch 19/20\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9178 - loss: 0.1873 - val_accuracy: 0.9026 - val_loss: 0.2288\n",
      "Epoch 20/20\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9187 - loss: 0.1946 - val_accuracy: 0.8828 - val_loss: 0.2979\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8796 - loss: 0.2875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8828\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Custom transformer for reshaping data for CNN input\n",
    "class ReshapeForCNN(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Reshape to (samples, time_steps, features)\n",
    "        return X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"output.csv\"  # Update this with your actual file path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Clean column names by stripping spaces\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Drop non-numeric columns\n",
    "non_numeric_cols = ['Device ID', 'Warm?', 'Sync', 'Arm', 'Timestamp', 'Pose', 'Locked', 'RSSI', 'newTimeStamp']\n",
    "df = df.drop(columns=non_numeric_cols, errors='ignore')\n",
    "\n",
    "# Encode the target labels\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['label'])  # Convert 'release' and 'fisk' to numbers\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(columns=['label'])\n",
    "y = df['label']\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Build the preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('features', scaler, X.columns)  # Apply scaling to all feature columns\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define a function to create the CNN model\n",
    "def create_cnn_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        \n",
    "        Conv1D(filters=128, kernel_size=3, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(len(np.unique(y)), activation='softmax')  # Output layer for classification\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess the data\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "# Reshape the data for CNN\n",
    "X_train_processed = X_train_processed.reshape(X_train_processed.shape[0], X_train_processed.shape[1], 1)\n",
    "X_test_processed = X_test_processed.reshape(X_test_processed.shape[0], X_test_processed.shape[1], 1)\n",
    "\n",
    "# Build and train the model\n",
    "model = create_cnn_model(input_shape=(X_train_processed.shape[1], 1))\n",
    "model.fit(X_train_processed, tf.keras.utils.to_categorical(y_train), epochs=20, batch_size=32, validation_data=(X_test_processed, tf.keras.utils.to_categorical(y_test)))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test_processed, tf.keras.utils.to_categorical(y_test))\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Save Model\n",
    "model.save(\"gesture_cnn_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)  # None will display all rows\n",
    "pd.set_option('display.max_columns', None)  # None will display all columns\n",
    "pd.set_option('display.width', None)  # Adjust width to fit all columns\n",
    "pd.set_option('display.max_colwidth', None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"dense_6\" is incompatible with the layer: expected axis -1 of input shape to have value 512, but received input with shape (32, 384)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(32, 21, 1), dtype=float32)\n  • training=False\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 61\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Store or use the processed data incrementally\u001b[39;00m\n\u001b[0;32m     58\u001b[0m X_list\u001b[38;5;241m.\u001b[39mappend(X_chunk)\n\u001b[1;32m---> 61\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_chunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# If you need to convert predictions back to original label encoding:\u001b[39;00m\n\u001b[0;32m     64\u001b[0m predicted_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(predictions, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\haris\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\haris\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\input_spec.py:227\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m axis, value \u001b[38;5;129;01min\u001b[39;00m spec\u001b[38;5;241m.\u001b[39maxes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m shape[axis] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[0;32m    224\u001b[0m             value,\n\u001b[0;32m    225\u001b[0m             \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    226\u001b[0m         }:\n\u001b[1;32m--> 227\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    228\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    229\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible with the layer: expected axis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    230\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof input shape to have value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    231\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut received input with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    232\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    233\u001b[0m             )\n\u001b[0;32m    234\u001b[0m \u001b[38;5;66;03m# Check shape.\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"dense_6\" is incompatible with the layer: expected axis -1 of input shape to have value 512, but received input with shape (32, 384)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(32, 21, 1), dtype=float32)\n  • training=False\n  • mask=None"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler # Assuming you're importing pandas for data loading\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# Set chunk size for reading the file incrementally\n",
    "chunk_size = 200 # Read 200 rows at a time\n",
    "\n",
    "# Initialize a counter to track the number of rows read\n",
    "rows_read = 0\n",
    "\n",
    "# Placeholder for processed data (you can choose to append or process incrementally as needed)\n",
    "X_list = []\n",
    "\n",
    "# Define non-numeric columns to drop\n",
    "non_numeric_cols = ['Device ID', 'Warm?', 'Sync', 'Arm', 'Timestamp', 'Pose', 'Locked', 'RSSI', 'newTimeStamp']\n",
    "\n",
    "# Define a function for processing each chunk\n",
    "def process_chunk(chunk, scaler):\n",
    "    # Clean column names\n",
    "    chunk.columns = chunk.columns.str.strip()\n",
    "\n",
    "    # Drop non-numeric columns\n",
    "    chunk = chunk.drop(columns=non_numeric_cols, errors='ignore')\n",
    "\n",
    "    # Normalize features\n",
    "    X = chunk  # Use the remaining columns as features\n",
    "    X_scaled = scaler.transform(X)  # Apply the scaler (assuming scaler is fit on training data)\n",
    "\n",
    "    # Reshape data for CNN (samples, time steps, features)\n",
    "    X_scaled = X_scaled.reshape(X_scaled.shape[0], X_scaled.shape[1], 1)  # Reshape to (samples, time_steps, features)\n",
    "\n",
    "    return X_scaled\n",
    "\n",
    "# Initialize the scaler (you can fit it on your training data before this step)\n",
    "scaler = StandardScaler()\n",
    "model = tf.keras.models.load_model(\"gesture_cnn_model.h5\")\n",
    "\n",
    "def get_mode(array):\n",
    "    mode_result = stats.mode(array)  # This returns an object with mode values and counts\n",
    "    mode = mode_result.mode[0]  # Corrected: access the mode value using .mode\n",
    "    return mode\n",
    "\n",
    "# Open the CSV and read in chunks\n",
    "while True:\n",
    "    for chunk in pd.read_csv('fisk_emg_filename.csv', chunksize=chunk_size):\n",
    "        # If this is the first chunk, fit the scaler on the data (using the training data ideally)\n",
    "        if rows_read == 0:\n",
    "            # Fit the scaler on the chunk (or better, fit on training data if available)\n",
    "            chunk.columns = chunk.columns.str.strip()\n",
    "            scaler.fit(chunk.drop(columns=non_numeric_cols, errors='ignore'))\n",
    "\n",
    "        # Process each chunk\n",
    "        X_chunk = process_chunk(chunk, scaler)\n",
    "\n",
    "        # Store or use the processed data incrementally\n",
    "        X_list.append(X_chunk)\n",
    "\n",
    "\n",
    "        predictions = model.predict(X_chunk)\n",
    "\n",
    "        # If you need to convert predictions back to original label encoding:\n",
    "        predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "        # Set NumPy to print full arrays without truncating\n",
    "        np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "        # Print the full predictions array\n",
    "        \n",
    "        print(stats.mode(predicted_labels))\n",
    "\n",
    "\n",
    "        # Print progress (optional)\n",
    "        rows_read += len(chunk)\n",
    "\n",
    "    # Convert the list to a numpy array for prediction\n",
    "    X_full = np.concatenate(X_list, axis=0)\n",
    "\n",
    "    # Now you can use X_full for model prediction\n",
    "    print(f\"Processed {rows_read} rows in total.\")\n",
    "\n",
    "    # Load the trained model\n",
    "\n",
    "\n",
    "    # Make predictions\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
